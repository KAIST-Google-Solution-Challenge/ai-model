{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C6qSa9pyIpI"
      },
      "source": [
        "# 1. Colab Setting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the directory path where training data and fine-tuned model files exist\n",
        "path = \"/content/drive/MyDrive/Solution Challenge/Algorithm/\""
      ],
      "metadata": {
        "id": "00lK0VKTeOJI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfAHX37TgJiP",
        "outputId": "cc803e33-eead-4d47-a80e-d6538cdd7861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk0oTMG-fepV",
        "outputId": "36615536-f2ae-4678-91f0-c3b7506ac23b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mxnet\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.9/dist-packages (from mxnet) (1.22.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.9/dist-packages (from mxnet) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.12)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.1\n",
            "    Uninstalling graphviz-0.20.1:\n",
            "      Successfully uninstalled graphviz-0.20.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.9.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gluonnlp\n",
            "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.5/344.5 KB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.4.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from gluonnlp) (1.22.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from gluonnlp) (0.29.33)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from gluonnlp) (23.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp39-cp39-linux_x86_64.whl size=680532 sha256=6f4cc2d32c376b115b97672242408a5ce041ae03048c7ad30ba096afba9092d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/17/70/b257bc53879a458c4bfcc900e89271aa8b4f19366a54bd2455\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==3.0.2\n",
            "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.0/769.0 KB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==3.0.2) (3.10.7)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.9/dist-packages (from transformers==3.0.2) (0.1.97)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==3.0.2) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==3.0.2) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==3.0.2) (2022.10.31)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from transformers==3.0.2) (23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from transformers==3.0.2) (1.22.4)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "  Downloading tokenizers-0.8.1rc1.tar.gz (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/97.4 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==3.0.2) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==3.0.2) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==3.0.2) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==3.0.2) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==3.0.2) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==3.0.2) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==3.0.2) (1.1.1)\n",
            "Building wheels for collected packages: tokenizers, sacremoses\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0m  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=13ebd19253927dfb50b204c137339262c6b8beb7cc3dce6c5d581931bf69123c\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
            "Successfully built sacremoses\n",
            "Failed to build tokenizers\n",
            "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers==3.0.2\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZvjiBjbf7xo",
        "outputId": "5dacb2d6-390e-4f2a-82a4-b498b24323d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-5do6ep9v\n",
            "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-5do6ep9v\n",
            "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3<=1.15.18\n",
            "  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 KB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from kobert==0.2.3) (0.10.0)\n",
            "Collecting mxnet<=1.7.0.post2,>=1.4.0\n",
            "  Downloading mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime<=1.8.0,==1.8.0\n",
            "  Downloading onnxruntime-1.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece<=0.1.96,>=0.1.6\n",
            "  Downloading sentencepiece-0.1.96-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch<=1.10.1,>=1.7.0\n",
            "  Downloading torch-1.10.1-cp39-cp39-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers<=4.8.1,>=4.8.1\n",
            "  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.22.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.19.6)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.9/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (23.3.3)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.19.0,>=1.18.18\n",
            "  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (23.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.33)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.9/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.27.1)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (0.8.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2022.10.31)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n",
            "Collecting sacremoses\n",
            "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.10.7)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n",
            "Collecting urllib3<1.26,>=1.20\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2022.12.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.16.0)\n",
            "Building wheels for collected packages: kobert\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15705 sha256=058b5741a1e8a140dfb81de467d2de7c8856944f9e291b8789e678c12ba1cd43\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8xsv46vo/wheels/0b/20/d8/031374f3d29b5150c59c814bed091fca7d6d4c8218148bf286\n",
            "Successfully built kobert\n",
            "Installing collected packages: tokenizers, sentencepiece, urllib3, torch, sacremoses, onnxruntime, jmespath, botocore, s3transfer, mxnet, huggingface-hub, transformers, boto3, kobert\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.97\n",
            "    Uninstalling sentencepiece-0.1.97:\n",
            "      Successfully uninstalled sentencepiece-0.1.97\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.15\n",
            "    Uninstalling urllib3-1.26.15:\n",
            "      Successfully uninstalled urllib3-1.26.15\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: mxnet\n",
            "    Found existing installation: mxnet 1.9.1\n",
            "    Uninstalling mxnet-1.9.1:\n",
            "      Successfully uninstalled mxnet-1.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.10.1 which is incompatible.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.10.1 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed boto3-1.15.18 botocore-1.18.18 huggingface-hub-0.0.12 jmespath-0.10.0 kobert-0.2.3 mxnet-1.7.0.post2 onnxruntime-1.8.0 s3transfer-0.3.7 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.10.3 torch-1.10.1 transformers-4.8.1 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gx0bnYFtf9QS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r1jr4un5gCi8"
      },
      "outputs": [],
      "source": [
        "# KoBERT\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "# Transformers\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UYwounGcgF1Q"
      },
      "outputs": [],
      "source": [
        "# Using GPU\n",
        "device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2awN7MogHoS",
        "outputId": "9bba99a3-9c71-4015-fa15-d02b846882af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/.cache/kobert_v1.zip[██████████████████████████████████████████████████]\n",
            "/content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# BERT model, load vocabulary\n",
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgPWEUgUyWds"
      },
      "source": [
        "# 2. Dataset Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IzRjomA1gOmN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "chatbot_data = pd.read_excel(path+'/conversation_data_set.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "beGQmlOQgbZ-"
      },
      "outputs": [],
      "source": [
        "data_list = []\n",
        "for q, label in zip(chatbot_data.iloc[:,1], chatbot_data.iloc[:,2])  :\n",
        "    data = []\n",
        "    data.append(q)\n",
        "    data.append(str(int(label)))\n",
        "\n",
        "    data_list.append(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N03sQnYigewF",
        "outputId": "cba7b8fb-81f9-4638-8377-fd15faa09e09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['여보세요 응 이런 사람은 알고 계세요. 건물이나 돈을 범죄 사겠다는 권고하는 과정에서 대량의 신용카드를 대포통령이 발견이 됐는데 그중 문제가 되는게 거기서 지금 명의로 된 하나은행 통장과 농협 통장에 발견이 됐어요. 그 두 개의 통장 개설에 대해서 아시는 내용 있으세요 아니요? 전혀 없어요음 그 두 개의 통장을 작년 3월 3일 경기도 광명시 철산동에서 바둑 계절을 개설된 계좌인데 어 전혀 모르셨어요음네 알겠습니다. 끝없이 통화 불편하시면 말씀을 해 주세요음 제출 계좌를 불법 현장에서 알 수 있기 때문에 동결 처리와 동시에 북구로 환수 조치했고요. 금융 운동은 제주공 결과 또 계좌번호도 확실히 명의 계정이한테 저희가 확인을 했어요. 제가 어 저에게 연락드린 이유는 어? 본인이 직접 통장을 개설하셔서 일당들에게 금전적인 대가를 받고 통장을 판매하신 건 아닌지 일단 아니면은 제 개인 정보가 유출이 되면서 명의 조정 당하실 피해자 분이신지에 대해서 저는 연락을 드린 거예요. 통장을 판매는 양봉하신 적 있으세요 아니요? 통장을 판매를 양도 하시는 것도 아니라고 보이는 하셨고도 모른다고 보기는 하셨는데 그러면은 사이에 지갑이나 핸드폰이랑 개인정보가 있을 만한 물건을 분실하신 데가 있을까요 아니요 없어요음 질문을 좀 똑바로 좀 해주세요 장난치지 마시고. 장난 아닌데요. 네, 알겠습니다. 자 그러면은 아 혹시 이런 쇼핑몰 사이트를 이용하세요 11번가나 옥션 지마켓. 라면 그냥 저희가 이걸 왜 못 잡았냐면 대부분 평소들이 이런 식품으로 사이트에서 개인 정보였지 상당히 많이 됐더라고요. 안녕! 네 저 지금 이거 조사를 어 한 두 시간 정도 할 건데 여기 괜찮으세요? 자.', '1']\n",
            "['서울중앙지방검찰청에서 전화드린 거예요네네 자 현재 저희 검찰에서 김용식 주범으로 인한 금융범죄 사기단의 검거한 상태예요. 자이 검거 현장에서 대량의 신용카드가 대포통장을 압수하던 과정에 연기로 돼 이제 농협 하나는 통장 두 점이 발견되어 조석제 연락드린 거고요. 자 문의 혹시이 통장 개별 거에 대해서 알고 계신 사실이 있으세요. 자 그러시면 본인이 개설하지 않았다고 말씀하시는 거죠. 그 피해자 분들이 현재 앞으로 고사고발 조치가 취해진 상태고요. 발생된 피해 금액은 약 2,300만 원. 정도고요음. 예 자 본인께서 금일 안으로 피해자 입증만 받게 되시면 본인 앞으로 취해진 고소 고발이나 폐 금액은 전부 지하철이 될 수 있으니까요. 어 너무 걱정하지 마시고 적극적으로 협조해 주세요. 몇 가지 질문 좀 할게요. 최근 3년에서 4년 사이 개인정보가 유출될 만한 물건이나 뭐 서류를 분실 또는 뭐 도난당하신 사례가 있으세요 자 없으시고 일단 질문을 바꿀게요 신분증 지갑 여권 핸드폰 신용카드 전혀 없으세요 자 그러시면 혹시 뭐 인터넷 금융권 통신사를 포함해서 뭐 개인정보 유출에 관한 전화 이메일 받아 보신 적 없으세요 자 본인 그거 뭐 인터넷 온라인 쇼핑몰 같은 거 이용하시죠 자 뭐 혹시 뭐 옥션이나 인터파크 이런데 이용하세요. 아 이용 안 하고 계시고 본인이 지금 너무 안일하게 생각하시는 거 같은데 확인 안 해 보시고 없다라고 말씀을 하시면 저희가 추가적인 저사 진행을 해 봐야 돼요. 본인 혹시 그럼 통신사 어디 있으세요? 자 지금 본인 명 본인과 연료된 명의동 사건이 있어요. 저희가 몇 가지 형인지 연락을 드린 거예요 소화 가능하세요 자 혹시 김용식이라고 혹시 본인 아는 사람이세요 자 다시 한번 물어볼게요. 전라도 광주 출신 42세 남성이에요 자 김용식이라고 처음 들어 보세요. 자 현재 검찰에서 김용식 주범으로 인한 금융 범죄 사장님의 검거한 상태예요이 공구 현장에서 대량의 신용카드가 이제 보통 잘 압수했는데 본인 명의로 된 통장 두 개가 발견되었어요. 이것도 연락을 드린 거고요. 자 본인 혹시 뭐이 통장 계속 거에 대해서 알고 계신 사실 있으세요?', '1']\n",
            "['네 일단 결과는 그렇게 나오셨거든요. 이해가 안가네요. 그 인제 말씀 드리자면 신용 대출 거래 내역을 토대로 이제 고객님께서 뭐 잘 이제 저희 쪽 마이너스 통장 발급 같은 경우 에는 내어 드렸을 때 원금과 이자되는 부분이 원활이 잘 상환을 해주실 수 있는지 뭐 이런 부분으로 이제 결과가 나오시는데 제가 봤을 때 고객님 충분히 가능하실거 같아서 발급이 그런 부분이 있을거 같아서 제가 이제 접수를 도와드렸던 부분인데, 어.. 은행에서는 지금 그런건가요? 심사과에서 그런건가요? 제가 마이너스 대출을 받으면 상환할 수 있는 능력이 없다고, 안된다고 판단하는 건가요 그럼? 약간 이제 그런 판단을 할 수 있는 게 판단을 할 수 있는 판단 여부 인제 그런 자료가 조금 약간 부족하다라는 식으로 이제 결과가 나오셨어요. 희한하네. 저희 쪽에서도 약간 이제 저한테는 충분히 사용 가능하실것 같아서 저도 상담 이제 접수 도와드렸던 부분이시고 제가 봤을 때는 건 부결 처리 나오신게 아쉬운 부분이 굉장히 많으세요. 지금 뭐 사대 보험 가입되어 있으시고 사대 보험도 있으신 직장도 재직 중 이신데다가 재직 기간도 오래 되셨고 소득 같은 부분도 그렇게 적으신 편이 아니신데 조금 아쉬운 부분이 좀 많으신데. 그니까 이게 왜,, 그냥 아예 안 된다고 나온건가요? 금액이 줄어 든게 아니라, 아예 마이너스 통장이 발급이 안된다고 나온건가요? 네 부결처리가 나오셨어요. 이 부분이 음,. 이해가 안가네. 제가 핸드폰 요금이나 카드 요금이나 이런 거는 한번도 밀린적이 없는데. 제가 지금 신용 대출을 받아도 제가 삼천만원 정도는 나오는 걸로 알고 있는데. 음 그러셨구나. 그럼 저 제가 봤을때도 조금 그런 부분이 아쉬운 부분이 좀 많으신데 아무래도 제가 생각하기에는 이제 서민 구제 명목으로 나온 거기 때문에.. 네. 고객님께서 선정이 안되셨나.. 그 부분에 대해서 조금.. 뭐라 말씀드려야 되지? 약간 조건이 너무 좋으셔서? 구제 명목이 아닌, 다른 쪽으로 이렇게 하셔서, 결과가이렇게 나오신 게 아니신가 생각이 되기는 조금 해요.', '1']\n",
            "['살아오면서 받은 선물 가운데서 최고의 기억에 남는 선물이 뭐예요? 최고로 좋았던 거. 최고로 좋았던 선물은 어 우리 아들 태어났을 때 하나님이 이렇게 귀한 아들을 나에게 주신 거에 대해서 너무 감사해서 얼마나 울었는지 몰라요. 왜냐면 우리 친정엄마는 음 아이들을 딸을 다섯을 낳고 아들을 낳었거든요. 그래서 우리 할머니한테 엄청 많이 음 음 조금 이렇게 구박당하는 걸 많이 어렸을 때 봤어요. 그래서 나는 시집가서 딸을 열을 낳아도 스물을 낳아도 아들 날 때까지 낳아야지 했는데 우리 아들을 첫 첫아들을 얻고 보니까 너무 큰 선물을 받은 거 같아서 그 선물을 우리 엄마한테 자랑하고 싶은 거예요. 근데 우리 엄마가 이 땅에 없어서 자랑을 못 하는 것이 아깝더라고요. 이 선물을 자랑하고 싶은 사람이 너무 많더라고요. 그래서 어 정말 우리 엄마한테만큼은 꼭 자랑하고 싶었던 선물인데 난 그때를 잊을 수가 없어요. 우리 아들이 태어난 날을 그래서 그때 우리 아들이 태어난 그 선물이 정말 하나님이 주신 큰 선물이라고 생각하고 여태 살아왔어요. name 씨는 어떤 때 가장 좋은 선물을 받았다고 생각해요? 보이는 선물 같은 건 별로 잘 안 해요. 뭐 남편이나 뭐 애들 생일 때나 뭐 그런 때 이렇게 생 그리고 저는 선물하는 거 같은 거 특별히 잘 안 한 거 같은데 안 하고 저는 안 하고 안 받은 게 좋다고는 좀 그런 주의예요. 근데 저도 가장 큰 선물은 제 인생 가운데 신앙이었던 거 같아요. 저는 고등학교 1학년 때부터 1학년 때 처음 제가 신앙생활을 시작했거든요. 교회를 다니기 시작했거든요. 가장 큰 선물은 신앙과 그 하나님 그러므 그러므로 인해서 제가 1학년 때 교회를 다니므로 인해서 우리 남 남동생을 전도하고 언니를 전도하고 그러므로 인해서 인저 저기 우리 가족의 그 신앙의 형 혈통들이 이어져 내려오는데 동네 친구들도 세 명을 전도했는데 쪼금 다니다가 말더라고요. 교회를 그 교회가 없었어요. 저희 동네에는 그래서 가장 큰 선물은 신앙 하나님 인생에서 그래서 저도 언니는 아들이 진짜 그러니까 얼 그 아들을 어 엄마한테 진짜 엄마가 그 엄마는 그렇게 딸을 많이 낳고 나중에 아들을 낳는데 얼마나 진짜 자랑하고 싶었을까? 엄마한테 그때는 할머니 살아 살아계셨었잖아요.', '0']\n",
            "['그 한국의 그 케이크 문화에 대해서 또 어떻게 생각하세요? 네 예전에는 그냥 뭐 생일이 주로 생일 잔치를 할 때 케익을 샀잖아요. 근데 요즘은 뭐 축하할 일이 있거나 뭐 좋은 일 있을 때도 케익을 많이 하는데 케익값이 너무나 천차만별이더라고요. 보통 우리 동네에서 살 때는 뭐 작은 거는 만 6천 원에서 만 8천 원댄데 이렇게 모양 이쁘게 좀 장식을 해서 하는 거는 10만 원대 가까이 되는 것도 있어서 굳이 저렇게까지 해야 되나 뭐 이 이런 마음도 들거든요. 근데 이제 어떻게 어 보기 좋은 떡이 먹기도 좋다고 이제 예쁘게 하는 것도 좋은데 정말 맛있는 것도 있겠지만 그렇지 못한 것도 있는 걸 보거든요. 그래서 그 그런 거는 복불복인 거 같고 보통 에스엔에스에 올리기 좋은 어 이미지를 많이 이렇게 선호하더라 하기 때문에 젊은 애들은 사진 찍어서 올리는 걸 이게 즐기더라고요. 그리고 제빵업체에서도 이제 그런 쪽으로 해야 조금 매출이 올라가기 때문에 이제 그런 쪽으로도 많이 발전이 되는 거 같고요. 그리고 이런 가격대가 다양하잖아요. 뭐 이렇게 10만 원대서부터 1 2만 원대까지 있는데 이런 가격 대에 대해서는 어떻게 생각하세요? 저는 이제 케익을 자주 사 본 적이 없어서 가격에 대해서는 저 잘 몰랐었는데요. 이제 지금 이 뉴스나 이런 매스컴을 보니까 이제 그 다양한 걸로는 알고 있는데 뭐 들어가는 재료나 뭐 그 여러 가지로 해서 이제 하 가격이 매겨지는 거 같긴 한데 이렇게 너무 하늘로 치솟 너무 하늘로 치솟게 되면 이제 이게 너무 너무 상업적으로 되지 않을까. 예 약간 이제 이제 소비자의 입장에서는 어차피 인제 자기가 이제 만족할만한 인제 그런 이렇게 사는 거긴 한데 이게 그 너무 상업적으로 되다 보니 업계끼리 전쟁이 되다 보니까 이제 너무 이제 상업화가 돼간다 이런 식으로 어떤 그 밸런 타인 데이 이 이런 거 비슷하다 할 수 있죠. 뭐 화 화이트데이 뭐 이런 사탕이나 뭐 약간 그 그렇게 변질이 된다면 조금 약간 그 조 좋은 모습이 되진 않을 거 같아가지고 약간 조금 가격은 조금 이렇게 최대한 저렴하게 했으 했으면 좋겠습니다. 예 그 케이크 문화가 어떤 방향으로 흘러갔으면 좋겠습니까? 예.', '0']\n",
            "['요즘 코로나 때문에 스포츠 모든 경기 관람이 많이 줄어들었잖아요. 그래서 티브이에서 에 에 스포츠 광 저기 관람석에 관중 없이 방송을 하더라고요. 근데 옛날만큼은 그 뭐라고 그래야 되지? 재미는 없는데 스포츠 이렇게 티브이 방송하는 거 자주 보세요? 저는 티브이 시청을 거의 하루에 한 시간도 안 하거든요. 그리고 그런데 뭐 이렇게 월드컵 하거나 뭐 올림픽 스포츠 뭐 그런 것도 자주 잘 안 봐서 보지는 않는데 잠깐 뭐 인터넷 검색해서 보기는 하는데 정말로 요즘에 코로나 때문에 음 텅 빈 그런 휑한 그걸 보기는 하고 참 운동선수들이 야구 같은 경우에는 관람석의 그 환호성이 있어야지 좀 어깨에 힘도 들어가고 좀 그럴 텐데 참 생뚱맞겠다. 그런 생각을 했어요. 혹시 스포츠 중에서 뭐 좋아하시는 스포츠가 있으신가요? 제가 뭐 직 접 좋아서 하는 거는 없고요. 그냥 저는 아들만 둘을 키우다 보니까 그냥 자연스럽게 옆에 앉아서 뭐 야구라든가 뭐 축구라든가 이런 스포츠 프로그램은 많이 본 거 같아요. 그리고 음 애들이랑 이렇게 같이 살 때는 스포츠 방송을 틀어놓으니까는 같이 봐서 경기 룰법 규칙도 알고 선수들도 이렇게 이름에 많이 알았었는데 이제 애들이 집에 없고 다 나가 있으니깐 저 혼자는 스포츠를 그 방송을 보게 안 되더라고요. 그래가지고 그냥 아 스포츠 하면 그냥 옛날에 봤던 거 그 정도만 기억하고 있어요. 혹시 좋아하시는 운동 있으세요? 제가 조금 약간 몸치예요. 그래서 운동을 제가 잘하질 못하다 보니까 스포츠를 보는 것도 별로 재미가 없더라고요. 근데 이제 아무래도 이제 나이가 먹으니까 운동을 해야 된다고 생각해서 이제 헬스를 하긴 하는데 거기서 뭐 지엑스로 뭐 요가랄지 그런 걸 하긴 하는데 쪼끔 즐겁지는 이렇게 운동을 해야지 좋은데 워낙 스포츠를 싫어하다 보니까 그냥 뜨문뜨문 가고 있어요. 근데 요즘에 그 헬스장도 코로나 때문에 사람들이 마스크를 쓰고 또 해요. 그래서 더 힘든 상황이 온 거 같아요. 어 최근에 어떤 운동 같은 거나 스포츠에 관심이 가져지는 게 따로 있으신가요? 저도 운동을 엄청 싫어해요.', '0']\n",
            "['병역 특례를 받아 이게 병역이 면제되는 예술가들이나 축구선수들이나 아니면 이렇게 국가대표 국가적으로 어쨌든 큰 기여를 했던 사람들이 네 병역 특례를 받지만 어쨌든 그 시간을 저기 한 어 음 보완하는 어쨌든 그러면서 어 봉사 시간을 채워야 되는 게 있는데 이걸 부풀려서 복무 규정을 위반한 사람들이 많은 것으로 나타났다고 하네요. 그래서 네 그래서 군 복무가 면제됐던 게 취소가 됐대요. 그래서 음 장현수라는 축구선수는 병역 특례를 받았다가 봉사 활동 시간을 부풀려서 신고한 사실이 적발돼서 대표팀 영구 제명과 대표팀 되려면 얼마나 이 국가를 대표하는 한 사람으로 뽑히기 위해서 얼마나 많은 시간과 노력과 이런 걸 다 들였을 텐데 영구 제명 아무리 노력해도 인제 다시 될 수 없는 거잖아요. 네 그리고 뭐 이렇게 벌금 받 은 그런 사실이 있다고 하는데 참 안타까운 사실이네요. 진짜 안타깝네요. 근데 지금 보니까 그 체육인들 봉사 활동 시간이 체육 요원의 봉사 활동 시간이 190 여섯 시간이래요. 근데 196시간을 여덟 시간 저희 뭐 어디 근무할 때 여덟 시간 근무니까 계산해 보니까 한 25일 정도거든요. 그래요? 여기 제가 본 신문에는 544시간으로 되어 있는데요. 그 인터넷 뉴스에서 봤었는데 그래서 체육 요원으로 봉사 활동 196시간을 채워야 하지만 장현수는 모교에서 봉사 활동 실적을 허위로 조작해서 제출해서 논란이 됐다 이렇게 나왔더라고요. 근데 196시간이면 그렇게 많은 시간은 아니잖아요. 이건 신문 기사라서 이게 더 정확하지 않을까요. 인터넷 뉴스는 쪼금 정확한 데가 정확한 정확한 게 좀 떨어 정확도가 떨어질 수가 있어서 신문 기사 상으로는 544시간이라고 나와 있네요. 그 이 정도는 돼줘야 그래도 봉사를 했다 하지 않을까요. 그래도 장현수 같은 경우는 자기가 있는 근무지에서 아주 먼 거리를 선택해서 그 이동 시간까지 계산해주니까 그렇게 해서 시간을 부풀렸다고 하더 하더라고요. 그러면 이 사람은 그 했 2014년 인천 아시안 게임에서 축구 금메달 딴 거 땜에 시간이 그럼 더 적었던 건가 보네요?', '0']\n"
          ]
        }
      ],
      "source": [
        "# Check few sentences for train and test\n",
        "print(data_list[0])\n",
        "print(data_list[400])\n",
        "print(data_list[800])\n",
        "print(data_list[1200])\n",
        "print(data_list[1600])\n",
        "print(data_list[2000])\n",
        "print(data_list[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ3YxqcoyfoY"
      },
      "source": [
        "# 3. Split Data into Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "I4T_iEbxghGt"
      },
      "outputs": [],
      "source": [
        "# Split data into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "                                                         \n",
        "dataset_train, dataset_test = train_test_split(data_list, test_size=0.25, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSGiv-kigkq2",
        "outputId": "45a36005-c5a9-4ef5-e8ee-526c2898033b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1645\n",
            "549\n"
          ]
        }
      ],
      "source": [
        "# Check the number of sentences\n",
        "print(len(dataset_train))\n",
        "print(len(dataset_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtrVnSkCymma"
      },
      "source": [
        "# 4. Prepare KoBERT and Input Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2Vas7XiegmUL"
      },
      "outputs": [],
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SHpzst6niY93"
      },
      "outputs": [],
      "source": [
        "# Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 10\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYMU8JwhiceF",
        "outputId": "e592c841-c79c-46eb-df82-3c1ed8670a56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        }
      ],
      "source": [
        "# Tokenization\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4jzin2qieLm",
        "outputId": "7a5da533-dcb2-4983-b069-4cd527844b88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WO_DODJywpA"
      },
      "source": [
        "# 5. Prepare KoBERT classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Oz0y_NuSilYW"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=2,   # Modify the number of classes\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzoV8CZMin07",
        "outputId": "4e057f96-ff75-4a37-e4f0-311193f3e289"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7feccc56d970>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Load KoBERT model\n",
        "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
        "\n",
        "# Set optimizer and schedule\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "# Define a function for accuracy measure\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "    \n",
        "train_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tl2Ntlxy7vM"
      },
      "source": [
        "# 6. Test New Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TUdE7BapdFAe"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained pt file\n",
        "model_conv = torch.load(\"/content/drive/MyDrive/KAIST/2022winter/voice_phishing/conversation_model_res.pt\")"
      ],
      "metadata": {
        "id": "eofy-pIyfBrG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt4KQmWrisvh",
        "outputId": "823821d1-6a5b-4ad5-d183-f179d05f806b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        }
      ],
      "source": [
        "# Tokenization\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "def softmax(a):\n",
        "    exp_a = np.exp(a)\n",
        "    sum_exp_a = np.sum(exp_a)\n",
        "    y = exp_a / sum_exp_a\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "def predict(predict_sentence):\n",
        "\n",
        "    data = [predict_sentence, '0']\n",
        "    dataset_another = [data]\n",
        "\n",
        "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
        "    \n",
        "    model_conv.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        out = model_conv(token_ids, valid_length, segment_ids)\n",
        "\n",
        "\n",
        "        test_eval=[]\n",
        "        for i in out:\n",
        "            logits=i\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            print(logits)\n",
        "            if np.argmax(logits) == 0:\n",
        "                test_eval.append(\"일반 대화\")\n",
        "            elif np.argmax(logits) == 1:\n",
        "                test_eval.append(\"보이스피싱\")\n",
        "            result = softmax(logits)\n",
        "            \n",
        "    result_list = result.tolist()\n",
        "\n",
        "    return result_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Prepare Server with Flask"
      ],
      "metadata": {
        "id": "V09g8PHHfKH0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZf6d83MmyiP",
        "outputId": "d3ca9faa-5528-4978-90ec-2a668cc65100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.9/dist-packages (2.2.3)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.9/dist-packages (from flask) (2.2.3)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from flask) (6.1.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.9/dist-packages (from flask) (2.1.2)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from flask) (3.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.9/dist-packages (from flask) (8.1.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6.0->flask) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2>=3.0->flask) (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install flask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV1v9U6siJLW",
        "outputId": "6c628bc0-1043-4f7e-cdc4-2f1ab8653d82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.2.1.tar.gz (761 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.3/761.3 KB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.2.1-py3-none-any.whl size=19790 sha256=b9325370b6599e58b2b0e7638d930d70a0f9112e54d920c8a05a6ca696f1d78f\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/89/59/49d4249e00957e94813ac136a335d10ed2e09a856c5096f95c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3zSb--xTqe7B"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, jsonify, request, Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "eooP1GK9m-dZ"
      },
      "outputs": [],
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/classify/')\n",
        "def classify():\n",
        "    # Handle GET request here\n",
        "    # sentence = request.get_json()\n",
        "    print(\"request recieved!\")\n",
        "    try:\n",
        "      # print(\"handle...\")\n",
        "      data = {'message': 'Hello, world!'}\n",
        "      # print(request.get_json())\n",
        "      sentence = request.get_json()\n",
        "      # print(\"data achieved!\")\n",
        "      # print(sentence)\n",
        "      # print(sentence[\"data\"])\n",
        "      data = predict(sentence[\"data\"])\n",
        "      print(\"predict success!\")\n",
        "      # print(sentence)\n",
        "    except Exception as e:\n",
        "      print(f\" Err: {e}\")\n",
        "      data = {'message': 'Hello, world!'}\n",
        "\n",
        "    \n",
        "    # data = {'message': 'Hello, world!'}\n",
        "    return jsonify(data[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACnOMmWqnIKf",
        "outputId": "b354185e-da61-49f7-de40-bbf8368fe65f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: http://b17d-34-143-209-155.ngrok.io\n"
          ]
        }
      ],
      "source": [
        "# Start ngrok tunnel\n",
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(port=80).public_url\n",
        "print(\"Public URL:\", public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOGxj1LAnJ4J",
        "outputId": "130f9954-bc03-4bf1-a267-7f8be18dd4dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:80\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Mar/2023 17:56:11] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Mar/2023 17:56:11] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Mar/2023 17:56:13] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        }
      ],
      "source": [
        "app.run(port=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A9rzKZ-nCDo",
        "outputId": "39f74a6b-7e93-47fc-9dd6-ddb4a12336fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public IP address of the Colab runtime: 172.28.0.12\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import socket\n",
        "import requests\n",
        "\n",
        "# Get the public IP address of the Colab runtime\n",
        "s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
        "s.connect((\"8.8.8.8\", 80))\n",
        "public_ip_address = s.getsockname()[0]\n",
        "s.close()\n",
        "\n",
        "# Print the IP address\n",
        "print(\"Public IP address of the Colab runtime:\", public_ip_address)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qPJDbEYnGGt",
        "outputId": "38683176-1294-4b3c-8a05-c0e9c4a0177c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2Bd65SUazYWfMaAm9a2LhqSRps8_4Seeutz6NQiD7v1S3PU2T"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}